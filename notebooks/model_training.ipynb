{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier Models and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import cfmining.models as models\n",
    "from cfmining.outlier_detector import *\n",
    "from cfmining.datasets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATIO = 1/10\n",
    "TEST_RATIO = 0.5\n",
    "SEED = 0\n",
    "n_jobs = 8\n",
    "n_trials = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_spaces = {\n",
    "    \"LogisticRegression\" : {\n",
    "        \"C\": {\"low\": 1e-5, \"high\": 1, \"log\": True, \"type\": \"float\"},\n",
    "        \"class_weight\": {\"choices\": [\"balanced\"], \"type\": \"categorical\"},\n",
    "        \"random_state\" : {\"choices\": [SEED], \"type\": \"categorical\"},\n",
    "        \"max_iter\" : {\"low\" : 1000, \"high\" : 1000, \"type\" : \"int\"},\n",
    "    },\n",
    "    \"LGBMClassifier\": {\n",
    "        \"n_estimators\": {\"low\": 5, \"high\": 250, \"type\": \"int\"},\n",
    "        \"learning_rate\": {\"low\": 0.05, \"high\": 1.0, \"type\": \"float\"},\n",
    "        \"max_depth\": {\"low\": 2, \"high\": 12, \"type\": \"int\"},\n",
    "        \"colsample_bytree\": {\"low\": 0.1, \"high\": 1.0, \"type\": \"float\"},\n",
    "        \"reg_lambda\": {\"low\": 1e-2, \"high\": 1e4, \"log\": True, \"type\": \"float\"},\n",
    "        \"verbose\": {\"choices\": [-1], \"type\": \"categorical\"},\n",
    "        \"random_state\" : {\"choices\": [SEED], \"type\": \"categorical\"},\n",
    "    },\n",
    "    \"MLPClassifier\": {\n",
    "        \"hidden_layer_sizes\": {\n",
    "            \"choices\": [\n",
    "                [30],\n",
    "                [30, 30],\n",
    "                [30, 30, 30],\n",
    "                [64],\n",
    "                [30, 64],\n",
    "            ],\n",
    "            \"type\": \"categorical\",\n",
    "        },\n",
    "        \"learning_rate_init\": {\"low\": 1e-5, \"high\": 1e-3, \"type\": \"float\", \"log\": True},\n",
    "        \"weight_decay\": {\"low\": 1e-5, \"high\": 1e-3, \"type\": \"float\", \"log\": True},\n",
    "        \"epochs\": {\"low\": 10, \"high\": 100, \"type\": \"int\", \"step\": 10},\n",
    "        \"class_weight\": {\"choices\": [\"balanced\"], \"type\": \"categorical\"},\n",
    "        \"batch_size\" : {\"low\" : 128, \"high\" : 128, \"type\" : \"int\"},\n",
    "        \"random_state\" : {\"choices\": [SEED], \"type\": \"categorical\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "def get_model(X, model_name, categorical_features, params):\n",
    "    num_features = [col for col in X.columns if col not in categorical_features]\n",
    "    model = [\n",
    "        (\"preprocess\", ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), num_features),\n",
    "            (\"cat\", OneHotEncoder(sparse_output=False), categorical_features)\n",
    "        ])),\n",
    "    ]   \n",
    "    if model_name == \"LGBMClassifier\":\n",
    "        model.append((\"classifier\", LGBMClassifier(**params)))\n",
    "    elif model_name == \"MLPClassifier\":\n",
    "        model.append((\"classifier\", models.MLPClassifier(**params)))\n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        model.append((\"classifier\", LogisticRegression(**params)))    \n",
    "    model = Pipeline(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(\n",
    "    trial,\n",
    "    hyperparams,\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_val,\n",
    "    Y_val,\n",
    "    model_name = \"LGBMClassifier\",\n",
    "    categorical_features = [],\n",
    "    ):\n",
    "    params = {}\n",
    "    for k, v in hyperparams.items():\n",
    "        if v[\"type\"] == \"categorical\":\n",
    "            params[k] = trial.suggest_categorical(k, v[\"choices\"])\n",
    "        elif v[\"type\"] == \"int\":\n",
    "            params[k] = trial.suggest_int(k, v[\"low\"], v[\"high\"])\n",
    "        elif v[\"type\"] == \"float\":\n",
    "            params[k] = trial.suggest_float(k, v[\"low\"], v[\"high\"], log=v.get(\"log\", False))\n",
    "\n",
    "    model = get_model(X_train, model_name, categorical_features, params)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "    score = balanced_accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LGBMClassifier\",\n",
    "    \"MLPClassifier\",\n",
    "]\n",
    "\n",
    "for dataset_name in [\"german\", \"german_cat\", \"taiwan\", \"taiwan_cat\", \"adult\", \"adult_cat\"]:\n",
    "\n",
    "    #### LOAD DATA\n",
    "    os.makedirs(f\"results/{dataset_name}\", exist_ok=True)\n",
    "    cat = \"_cat\" in dataset_name\n",
    "\n",
    "    dataset = DATASETS_[dataset_name.replace(\"_cat\", \"\")](use_categorical=cat)\n",
    "    X, Y = dataset.load_data()\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_RATIO, random_state=SEED, shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=VAL_RATIO, random_state=SEED, shuffle=True)\n",
    "\n",
    "    # reset index\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    Y_train = Y_train.reset_index(drop=True)\n",
    "    Y_val = Y_val.reset_index(drop=True)\n",
    "    Y_test = Y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    #### HYPERPARAMETER OPTIMIZATION OF EACH MODEL\n",
    "    for model_name in model_list:\n",
    "        study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        )\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, hyperparam_spaces[model_name], X_train, Y_train, X_val, Y_val, model_name, dataset.categoric_features),\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs if model_name != \"MLPClassifier\" else 1,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        params = study.best_params\n",
    "        model = get_model(X_train, model_name, dataset.categoric_features, params)\n",
    "        \n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_train_pred = model.predict(X_train)\n",
    "        Y_test_pred = model.predict(X_test)\n",
    "\n",
    "        print(f\"Number of denied samples from test: {(1 - Y_test_pred).sum():.0f}\")\n",
    "        print(f\"Score  training: {balanced_accuracy_score(Y_train, Y_train_pred):.3f}\")\n",
    "        print(f\"Score validation: {study.best_value:.3f}\")\n",
    "        print(f\"Score test: {balanced_accuracy_score(Y_test, Y_test_pred):.3f}\")\n",
    "\n",
    "        joblib.dump(model, f\"../models/{dataset_name}/{model_name}.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "    #### SAVE ISOLATION FOREST\n",
    "    outlier_detection = IsolationForest(contamination = dataset.outlier_contamination, ndim=1, sample_size=0.25, max_depth=\"auto\", ntrees=100, missing_action=\"divide\")\n",
    "    outlier_detection.fit(X_train);\n",
    "    joblib.dump(outlier_detection, f\"../models/{dataset_name}/IsolationForest.pkl\")\n",
    "\n",
    "    outlier_detection = IsolationForest(dataset.outlier_contamination, ndim=2, sample_size=0.25, max_depth=\"auto\", ntrees=100)\n",
    "    outlier_detection.fit(X_test);\n",
    "    joblib.dump(outlier_detection, f\"../models/{dataset_name}/IsolationForest_test.pkl\")\n",
    "\n",
    "\n",
    "    #### SAVE AE OUTLIER DETECTOR\n",
    "    outlier_detection = AE_OutlierDetector(\n",
    "        categoric_features=dataset.categoric_features,\n",
    "        contamination=dataset.outlier_contamination,\n",
    "        hidden_layer_sizes = [128, 64, 64, 64],\n",
    "    )\n",
    "    outlier_detection.fit(X_train);\n",
    "    joblib.dump(outlier_detection, f\"../models/{dataset_name}/AE_OutlierDetection.pkl\")\n",
    "\n",
    "    outlier_detection = AE_OutlierDetector(\n",
    "        categoric_features=dataset.categoric_features,\n",
    "        contamination=dataset.outlier_contamination,\n",
    "        hidden_layer_sizes = [128, 64, 64, 64],\n",
    "    )\n",
    "    outlier_detection.fit(X_test);\n",
    "    joblib.dump(outlier_detection, f\"../models/{dataset_name}/AE_OutlierDetection_test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
